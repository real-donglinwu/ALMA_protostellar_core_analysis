{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note by the author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:1.3em;\">\n",
    "This code is written by: Donglin Wu\n",
    "<br>\n",
    "If you found any errors, please contact: donglin.wu@yale.edu\n",
    "<br>\n",
    "\n",
    "This is a preliminary version of the code, so there are places you need to input and modify throughout the code.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:1.3em;\">\n",
    "First, you need to input the directory and name of the fits cube, the directory and name of the output files, center of the desired images (usually taken to be somewhere near the continuum emission), the distance of the source, and other source-related figures.<br>\n",
    "\n",
    "\n",
    "Second, since sometimes fits cube might have degenerate axes (such as CARMA-NRO data), you might need to modify the WCS axes and any part of the code that has <font color=\"blue\"><em>cube_data.shape</em></font>.<br>\n",
    "\n",
    "Third, the most important function is <font color=\"red\">fit_gaussians</font>. It has many input parameters. Each of the parameters is important for the algorithm to operate correctly, and is described in details. Please read the descriptions carefully to select the parameters.<br>\n",
    "\n",
    "Fourth, for the velocity maps and velocity gradients, the important input is the <font color=\"blue\">region</font> over which the gradients are calculated/plotted/averaged.<br>\n",
    "\n",
    "Fifth, the intensity-weighted velocity map is only one of the ways to generate velocity maps. It is selected because it can alleviate the consequences caused by overfitting (especially for data with high velocity resolution such as ALMA). There can be other ways to find the velocity maps. I provided some alternative functions but they are not the only ones.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import fits cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy import constants as const\n",
    "from astropy import units as u\n",
    "import math\n",
    "from astropy.io import fits\n",
    "from astropy.wcs import WCS\n",
    "# from astropy.utils.data import get_pkg_data_filename\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from astropy.coordinates import SkyCoord  \n",
    "from astropy.coordinates import FK5  \n",
    "# from photutils.aperture import SkyEllipticalAperture, SkyRectangularAperture\n",
    "# from photutils.aperture import aperture_photometry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## [INPUT] required in this block\n",
    "## Input the directory and name of the fits cube\n",
    "\n",
    "filename = '' #[Input]\n",
    "\n",
    "hdul = fits.open(filename)\n",
    "hdul.info()\n",
    "\n",
    "hdu = hdul[0]\n",
    "cube_header = hdu.header\n",
    "cube_data = hdu.data\n",
    "\n",
    "## May require modification: change to the velocity axis and space axes correspondingly\n",
    "wcs = WCS(cube_header)\n",
    "wcs2d = wcs[0,:,:] \n",
    "wcsv = wcs[:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## [INPUT] required in this block\n",
    "## Input the coordinate of the center (of the conitnuum)\n",
    "\n",
    "center = SkyCoord(\"5:35:22.2008948674 -6:13:06.1608008323\", frame=FK5, unit=(u.hourangle, u.deg))  #[Input]\n",
    "center_pixel = wcs2d.world_to_pixel(center)\n",
    "print(center, center_pixel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## May require modification: cube_data.shape[0] should be changed to the velocity axis\n",
    "v_world = np.array([wcsv.pixel_to_world(int(i)).value/1000 for i in range(cube_data.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcenter = int(center_pixel[0])\n",
    "ycenter = int(center_pixel[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## May require modification: change to the image axes correspondingly\n",
    "bmaj, bmin = cube_header['BMAJ'], cube_header['BMIN'] # beam major and minor axis\n",
    "dx, dy = cube_header['CDELT1'], cube_header['CDELT2'] # pixel size\n",
    "print(bmaj, bmin, dx, dy)\n",
    "print(bmaj/dx, bmin/dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## [INPUT] required in this block\n",
    "## Input the distance of the source\n",
    "\n",
    "d_source = 386 # 386.0 for HOPS 198\n",
    "\n",
    "\n",
    "deg_to_pc = d_source*math.pi/180\n",
    "deg_to_cm = u.parsec.to(u.cm)*d_source*math.pi/180\n",
    "\n",
    "abs(dx)*deg_to_pc, abs(dy)*deg_to_pc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit multiple gaussians to spectrum of each pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask of the image: select pixels with non-nan values\n",
    "mask_image = np.zeros(shape=(cube_data.shape[1],cube_data.shape[2]))\n",
    "mask_image[np.isnan(cube_data[50,:,:]) == False] = True\n",
    "mask_image[np.isnan(cube_data[50,:,:]) == True] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mask_image, origin='lower')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def noise_from_spectrum(spectrum, default_noise):\n",
    "#     noise = []\n",
    "#     for i in range (len(spectrum)):\n",
    "#         if i<default_noise[0] or i>default_noise[1]:\n",
    "#             if math.isnan(spectrum[i]) == False:\n",
    "#                 noise.append(spectrum[i])\n",
    "#     return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# Define a function for a single Gaussian\n",
    "def gaussian(x, amp, mean, stddev):\n",
    "    return amp * np.exp(-((x - mean) ** 2) / (2 * stddev ** 2))\n",
    "\n",
    "# Define a function for multiple Gaussians\n",
    "# Parameters should be given in list: [amp1, mean1, stddev1, amp2, mean2, stddev2, ...]\n",
    "def multiple_gaussians(x, *params):\n",
    "    n_gaussians = len(params) // 3\n",
    "    y = np.zeros_like(x)\n",
    "    for i in range(n_gaussians):\n",
    "        amp = params[i * 3]\n",
    "        mean = params[i * 3 + 1]\n",
    "        stddev = params[i * 3 + 2]\n",
    "        y += gaussian(x, amp, mean, stddev)\n",
    "    return y\n",
    "\n",
    "# Improved initial guess by finding peaks\n",
    "def initial_guess_from_peaks(x, y, n_components, distance):\n",
    "    # This function finds a certain number of peaks from a spectrum, where the number of peaks is specified as input: n_components\n",
    "    # distance: int, minimum separation between peaks\n",
    "    peaks, _ = find_peaks(y, distance=distance)\n",
    "    sorted_peaks = sorted(peaks, key=lambda p: y[p], reverse=True)[:n_components]\n",
    "    initial_params = []\n",
    "    for peak in sorted_peaks:\n",
    "        std1 = np.sqrt(-pow(x[peak-1]-x[peak], 2)/(2*np.log(y[peak-1]/y[peak])))\n",
    "        std2 = np.sqrt(-pow(x[peak+1]-x[peak], 2)/(2*np.log(y[peak+1]/y[peak])))\n",
    "        initial_params.extend([\n",
    "            y[peak],            # amplitude\n",
    "            x[peak],            # mean\n",
    "            (std1+std2)/2       # stddev, a rough guess\n",
    "        ])\n",
    "    return initial_params\n",
    "\n",
    "# Calculate criteria (AIC, BIC, AICc, DIC)\n",
    "def calculate_criterion(gmm, x, y, criterion='aic'):\n",
    "    X = np.array(list(zip(x, y)))\n",
    "    n = len(x)\n",
    "    k = gmm.n_components * 3 - 1  # Number of parameters in the model\n",
    "    \n",
    "    if criterion == 'aic':\n",
    "        return gmm.aic(X)\n",
    "    elif criterion == 'bic':\n",
    "        return gmm.bic(X)\n",
    "    elif criterion == 'aicc':\n",
    "        aic = gmm.aic(X)\n",
    "        aicc = aic + (2 * k * (k + 1)) / (n - k - 1)\n",
    "        return aicc\n",
    "    elif criterion == 'dic':\n",
    "        deviance = -2 * gmm.score(X)\n",
    "        dic = deviance + 2 * k\n",
    "        return dic\n",
    "    else:\n",
    "        raise ValueError(\"Criterion must be 'aic', 'bic', 'aicc', or 'dic'.\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit multiple Gaussian models and select the best one based on the chosen criterion\n",
    "def fit_gaussians(x, y, max_components=3, mean_range=(-np.inf, np.inf), noise_region=[0,-1], amp_threshold=1000,distance=3, criterion='aic', n_init=10):\n",
    "    # Input:\n",
    "        # x: 1d array\n",
    "        #   the velocity or frequency axis for the spectrum\n",
    "        #\n",
    "        # y: 1d array\n",
    "        #   the spectrum\n",
    "        #\n",
    "        # max_components: int\n",
    "        #   maximum number of components fitted to the spectrum\n",
    "        #\n",
    "        # mean_range: a tuple or list\n",
    "        #   the allowed range for the means of the components to be;\n",
    "        #   given as (min_mean, max_mean), which correspond to the minimum allowed value and maximum allowed value for the mean of the components;\n",
    "        #   if unspecified, assume no constraint on the means of the components\n",
    "        #\n",
    "        # noise_region: a list or tuple\n",
    "        #   the indices that specify the region of spectrum that is used to find the rms value to constrain how small the amplitude of the components can be;\n",
    "        #   given as a list or a tuple [index_min, index_max], this means y[:index_min] and y[index_max:] only consist of noise\n",
    "        #   if unspecified, assume no constraint on the amplitude of the components\n",
    "        #\n",
    "        # amp_threshold: float\n",
    "        #   the maximum value allowed fro the amplitude of the components\n",
    "        #   should be selected according to the level of the spectrum\n",
    "        #\n",
    "        # distance: int\n",
    "        #   minimum separation allowed for the peaks, which should be adjusted according to velocity/frequency resolution of the data;\n",
    "        #   a small distance of 1 to 3 should be set for data with low velocity resolution; a larger distance such as 5 or 8 should be set otherwise\n",
    "        #\n",
    "        # criterion: string, chosen from ['aic', 'bic', 'aicc', 'dic']\n",
    "        #   the information criterion used for determining the most suitable number of components fitted to the spectrum;\n",
    "        #   for spectrum with few effective data points (those that are above 3 rms), aicc should be used; otherwise, aic should be used\n",
    "        # \n",
    "        # n_init: int\n",
    "        #   number of initializations for the gaussian mixture to determine the most suitable number of components fitted to the spectrum;\n",
    "        #   strongly affects the time taken to complete the algorithm\n",
    "        #   usually 1 or 10 is enough\n",
    "\n",
    "    # Initiation\n",
    "    best_criterion = np.inf\n",
    "    best_gmm = None\n",
    "\n",
    "    # Find the noise level using the input noise_region\n",
    "    rms = np.nanstd(np.concatenate((y[:noise_region[0]], y[noise_region[1]:])))\n",
    "\n",
    "    # Find the region in the spectrum that is above 3 rms\n",
    "    mask_y = np.copy(y)\n",
    "    mask_y[mask_y < 3*rms] = np.nan\n",
    "    index_notnan = [i for i in range(len(mask_y)) if np.isnan(mask_y[i]) == False]\n",
    "    index_notnan_min, index_notnan_max = min(index_notnan), max(index_notnan)+1\n",
    "\n",
    "    # Find the most suitable number of components using Gaussian mixture and selected information criterion\n",
    "    for n_components in range(1, max_components + 1):\n",
    "\n",
    "        # Make the initial centers of the gaussian mixtures at the peaks of the spectrum\n",
    "        # If you feel like the function initial_guess_from_peaks does not find the suitable peaks, please comment the following four lines,\n",
    "        #   and delete means_init=mean_init from the sixth line below\n",
    "        mean_init = []\n",
    "        param_array = np.array(initial_guess_from_peaks(x,y,n_components, distance=distance))\n",
    "        for i in range(len(param_array)//3):\n",
    "            mean_init.append([param_array[i*3+1], param_array[i*3]])\n",
    "\n",
    "        gmm = GaussianMixture(n_components=n_components, covariance_type='diag', init_params='k-means++', means_init=mean_init, n_init=n_init)\n",
    "        X = np.array(list(zip(x[index_notnan_min:index_notnan_max], y[index_notnan_min:index_notnan_max])))\n",
    "        gmm.fit(X)\n",
    "\n",
    "        current_criterion = calculate_criterion(gmm, x[index_notnan_min:index_notnan_max], y[index_notnan_min:index_notnan_max], criterion)\n",
    "\n",
    "        if current_criterion < best_criterion:\n",
    "            best_criterion = current_criterion\n",
    "            best_gmm = gmm\n",
    "\n",
    "\n",
    "    # Use the peaks found from the function initial_guess_from_peaks for initial guesses of the fit\n",
    "    if best_gmm.n_components <= max_components:\n",
    "        initial_params = initial_guess_from_peaks(x, y, best_gmm.n_components, distance=distance)\n",
    "    else:\n",
    "        initial_params = []\n",
    "        for i in range(best_gmm.n_components):\n",
    "            initial_params.extend([\n",
    "                best_gmm.weights_[i] * y.max(),  # amplitude\n",
    "                best_gmm.means_[i, 0],           # mean\n",
    "                np.sqrt(best_gmm.covariances_[i, 0])  # stddev\n",
    "            ])\n",
    "\n",
    "    # Ensure initial params are within bounds\n",
    "    lower_bounds = []\n",
    "    upper_bounds = []\n",
    "    for i in range(len(initial_params) // 3):\n",
    "        lower_bounds.extend([0, mean_range[0], 1e-6])\n",
    "        upper_bounds.extend([amp_threshold, mean_range[1], np.inf])\n",
    "\n",
    "    initial_params = np.array(initial_params)\n",
    "    initial_params[np.isnan(initial_params) == True] = 0.5\n",
    "\n",
    "    initial_params = np.minimum(np.maximum(initial_params, lower_bounds), upper_bounds)  # Ensure p0 is within bounds\n",
    "\n",
    "\n",
    "    # Curve fitting with constraints\n",
    "    params, _ = curve_fit(multiple_gaussians, x, y, p0=initial_params, bounds=(lower_bounds, upper_bounds), maxfev=10000, nan_policy='omit')\n",
    "\n",
    "    filtered_params = []\n",
    "    for i in range(best_gmm.n_components):\n",
    "        amp = params[i * 3]\n",
    "        if amp > rms*3:\n",
    "            filtered_params.extend(params[i * 3:i * 3 + 3])\n",
    "\n",
    "    return len(filtered_params) // 3, filtered_params\n",
    "\n",
    "    # Output: int and a list\n",
    "        # len(filtered_params) // 3: int\n",
    "        #   number of components best fitted for this spectrum\n",
    "        #\n",
    "        # filtered_params: a list\n",
    "        #   parameters for function multiple_gaussians\n",
    "        #   should be in the format: [amp1, mean1, stddev1, amp2, mean2, stddev2, ...]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with one pixel in the fits cube\n",
    "\n",
    "spectrum = cube_data[:,184,179]\n",
    "n_components, params = fit_gaussians(v_world, spectrum, max_components=4, noise_region=[25,130], mean_range=(3.5, 12.5),criterion='aic', n_init=10)\n",
    "\n",
    "print(f\"Number of Gaussian components: {n_components}\")\n",
    "print(\"Parameters (amplitude, mean, stddev) for each Gaussian:\")\n",
    "for i in range(n_components):\n",
    "    print(f\"Gaussian {i + 1}: Amplitude={params[i * 3]}, Mean={params[i * 3 + 1]}, Stddev={params[i * 3 + 2]}\")\n",
    "\n",
    "\n",
    "# Plot the data and the fitted curve\n",
    "# plt.scatter(v_carma, spectrum, label='Data', s=10)\n",
    "plt.plot(v_world, spectrum)\n",
    "plt.plot(v_world, multiple_gaussians(v_world, *params), label='Fitted', color='red')\n",
    "plt.xlabel(r'$v_{\\text{LSR}}$ [km s$^{-1}$]')\n",
    "plt.ylabel(r'Intensity [Jy/beam]')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove warnings (since there will be a lot)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"divide by zero encountered in divide\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in scalar divide\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in scalar multiply\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in cast\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"Degrees of freedom <= 0 for slice\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## [INPUT] required in this block\n",
    "## Input the parameters of fit_gaussians\n",
    "\n",
    "data_ncomponents = np.zeros(shape=(cube_data.shape[1],cube_data.shape[2]),dtype='int')\n",
    "data_A = np.zeros(shape=(cube_data.shape[1],cube_data.shape[2]),dtype='object')\n",
    "data_mu = np.zeros(shape=(cube_data.shape[1],cube_data.shape[2]),dtype='object')\n",
    "data_sigma = np.zeros(shape=(cube_data.shape[1],cube_data.shape[2]),dtype='object')\n",
    "# data_error = np.zeros(shape=(cube_data.shape[1],cube_data.shape[2]))\n",
    "\n",
    "\n",
    "for xchan in range(cube_header['NAXIS1']):\n",
    "    for ychan in range(cube_header['NAXIS2']):\n",
    "        if mask_image.astype(int)[ychan,xchan] == 1:\n",
    "            try:\n",
    "                spectrum = cube_data[:, ychan, xchan]\n",
    "                n_components, params = fit_gaussians(v_world, spectrum, max_components=3, noise_region=[25,130], mean_range=(3.5, 12.5),criterion='aic', n_init=10)\n",
    "                data_ncomponents[ychan, xchan] = n_components\n",
    "                data_A[ychan, xchan], data_mu[ychan, xchan], data_sigma[ychan, xchan] = [], [], []\n",
    "                for i in range(n_components):\n",
    "                    #print(f\"Gaussian {i + 1}: Amplitude={params[i * 3]}, Mean={params[i * 3 + 1]}, Stddev={params[i * 3 + 2]}\")\n",
    "                    data_A[ychan, xchan].append(params[i * 3])\n",
    "                    data_mu[ychan, xchan].append(params[i * 3 + 1])\n",
    "                    data_sigma[ychan, xchan].append(params[i * 3 + 2])\n",
    "            except: \n",
    "                data_A[ychan, xchan], data_mu[ychan, xchan], data_sigma[ychan, xchan] = [np.nan], [np.nan], [np.nan]\n",
    "        if xchan % 50 == 0 and ychan % 50 == 0:\n",
    "            print(xchan, ychan)\n",
    "\n",
    "data_mu[data_mu == 0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "## [INPUT] required in this block\n",
    "## Input the directory and name of the output files\n",
    "\n",
    "dir_output = ''\n",
    "\n",
    "# Save parameters of the components in pickle\n",
    "output = open(dir_output+'/ALMA_13CO_data_mu.pkl', 'wb')\n",
    "pickle.dump(data_mu, output)\n",
    "output.close()\n",
    "\n",
    "output = open(dir_output+'/ALMA_13CO_data_A.pkl', 'wb')\n",
    "pickle.dump(data_A, output)\n",
    "output.close()\n",
    "\n",
    "output = open(dir_output+'/ALMA_13CO_data_sigma.pkl', 'wb')\n",
    "pickle.dump(data_sigma, output)\n",
    "output.close()\n",
    "\n",
    "# Save number of components for each pixel in fits file\n",
    "from astropy.io import fits\n",
    "\n",
    "hdu_output = fits.PrimaryHDU()\n",
    "hdu_output.data = data_ncomponents\n",
    "\n",
    "for i in range(len(list(hdu.header.keys()))):\n",
    "    key = list(hdu.header.keys())[i]\n",
    "    if '3' not in key and '4' not in key and 'COMMENT' not in key and 'HISTORY' not in key and key !='':\n",
    "        # print(key)\n",
    "        hdu_output.header.update({key:hdu.header[key]})\n",
    "\n",
    "hdu_output.header.update({'NAXIS':2})\n",
    "hdu_output.header['BUNIT'] = ''\n",
    "hdu_output.writeto(dir_output+'/ALMA_13CO_n_component.fits', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the velocity maps and velocity gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Velocity maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "\n",
    "# Find the component with minimum velocity in a certain velocity range\n",
    "def func_first_component_min(mu_ls, range_v):\n",
    "    indices = [i for i in range(len(mu_ls)) if range_v[0] <= mu_ls[i] <= range_v[1]]\n",
    "    if len(indices) > 1:\n",
    "        mu_ls_range = [mu_ls[index_i] for index_i in indices]\n",
    "        index_first = indices[np.argmin(mu_ls_range)]\n",
    "        return mu_ls[index_first], index_first\n",
    "    else:\n",
    "        return mu_ls[indices[0]], indices[0]\n",
    "\n",
    "# Find the component with maximum velocity in a certain velocity range\n",
    "def func_first_component_max(mu_ls, range_v):\n",
    "    indices = [i for i in range(len(mu_ls)) if range_v[0] <= mu_ls[i] <= range_v[1]]\n",
    "    if len(indices) > 1:\n",
    "        mu_ls_range = [mu_ls[index_i] for index_i in indices]\n",
    "        index_first = indices[np.argmax(mu_ls_range)]\n",
    "        return mu_ls[index_first], index_first\n",
    "    else:\n",
    "        return mu_ls[indices[0]], indices[0]\n",
    "    \n",
    "# Find the component with maximum intensity in a certain velocity range\n",
    "def func_brightest_component(mu_ls, v_world, spectrum, range_v):\n",
    "    indices = [i for i in range(len(mu_ls)) if range_v[0] <= mu_ls[i] <= range_v[1]]\n",
    "    mu_ls_range = [mu_ls[index_i] for index_i in indices]\n",
    "    spec_interp = interp1d(v_world, spectrum)\n",
    "    A_ls_range = [spec_interp(mu_ls[index_i]) for index_i in indices]\n",
    "    index_brightest = indices[np.argmax(A_ls_range)]\n",
    "    return mu_ls_range[index_brightest], index_brightest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"font-size:1.2em;\"> The following velocity map is the intensity-weighted velocity map. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## [INPUT] required in this block\n",
    "## Input mu_low and mu_high, which specify the range of the components we want to analyze\n",
    "\n",
    "mu_low, mu_high = 5, 6\n",
    "\n",
    "# mu_ave_peak: the intensity-weighted velocity \n",
    "# A_ave_peak: the average of the amplitude of the components\n",
    "# sigma_ave_peak: the sum of the velocity dispersions of the components\n",
    "\n",
    "mu_ave_peak = np.zeros(shape=(cube_data.shape[1],cube_data.shape[2]),dtype='float')\n",
    "A_ave_peak = np.zeros(shape=(cube_data.shape[1],cube_data.shape[2]),dtype='float')\n",
    "sigma_ave_peak = np.zeros(shape=(cube_data.shape[1],cube_data.shape[2]),dtype='float')\n",
    "\n",
    "# difference_mu_peaks: difference in velocity of the components (higher minus lower)\n",
    "# difference_A_peaks: difference in intensity of the components (the one with higher velocity minus the one with lower velocity)\n",
    "# difference_sigma_peaks: difference in velocity of the components (the one with higher velocity minus the one with lower velocity)\n",
    "difference_mu_peaks = np.zeros(shape=(cube_data.shape[1],cube_data.shape[2]),dtype='float')\n",
    "difference_A_peaks = np.zeros(shape=(cube_data.shape[1],cube_data.shape[2]),dtype='float')\n",
    "difference_sigma_peaks = np.zeros(shape=(cube_data.shape[1],cube_data.shape[2]),dtype='float')\n",
    "\n",
    "\n",
    "\n",
    "for xchan in range(cube_header['NAXIS1']):\n",
    "    for ychan in range(cube_header['NAXIS2']):\n",
    "        try:\n",
    "            if len(data_mu[ychan, xchan]) > 0:\n",
    "                mu_ls, A_ls, sigma_ls = [], [], []\n",
    "                for j in range(len(data_mu[ychan, xchan])):\n",
    "                    if mu_low < data_mu[ychan, xchan][j] < mu_high: #and data_A[ychan, xchan][j] > 0.3:\n",
    "                        mu_ls.append(data_mu[ychan, xchan][j])\n",
    "                        A_ls.append(data_A[ychan, xchan][j])\n",
    "                        sigma_ls.append(data_sigma[ychan, xchan][j])\n",
    "                # mu_ave_peak[ychan, xchan] = np.nanmean(np.array(mu_ls))\n",
    "                mu_ave_peak[ychan, xchan] = np.nansum(np.array(mu_ls)*np.array(A_ls))/np.nansum(np.array(A_ls))\n",
    "                A_ave_peak[ychan, xchan] = np.nanmean(np.array(A_ls))\n",
    "                sigma_ave_peak[ychan, xchan] = np.nansum(np.array(sigma_ls))\n",
    "                if len(mu_ls) >= 2:\n",
    "                    difference_mu_peaks[ychan, xchan] = max(mu_ls) - min(mu_ls)\n",
    "                    difference_A_peaks[ychan, xchan] = A_ls[np.argmax(np.array(mu_ls))] - A_ls[np.argmin(np.array(mu_ls))]\n",
    "                    difference_sigma_peaks[ychan, xchan] = sigma_ls[np.argmax(np.array(mu_ls))] - sigma_ls[np.argmin(np.array(mu_ls))]\n",
    "                \n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_ave_peak[mu_ave_peak == 0] = np.nan\n",
    "plt.subplot(projection=wcs2d)\n",
    "plt.imshow(mu_ave_peak, origin='lower',vmin=mu_low,vmax=mu_high,cmap='coolwarm')\n",
    "plt.colorbar(label=r'$v_{\\text{LSR, 1}}$ [km/s]')\n",
    "plt.xlabel('RA')\n",
    "plt.ylabel('DEC')\n",
    "plt.xlim(90,448-90)\n",
    "plt.ylim(90,448-90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_ave_peak[A_ave_peak == 0] = np.nan\n",
    "plt.subplot(projection=wcs2d)\n",
    "plt.imshow(A_ave_peak, origin='lower',vmax=0.5)\n",
    "plt.colorbar(label=r'$A_{1}$')\n",
    "plt.xlabel('RA')\n",
    "plt.ylabel('DEC')\n",
    "plt.xlim(90,448-90)\n",
    "plt.ylim(90,448-90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_ave_peak[sigma_ave_peak == 0] = np.nan\n",
    "plt.subplot(projection=wcs2d)\n",
    "plt.imshow(sigma_ave_peak, origin='lower',vmax=1)\n",
    "plt.colorbar(label=r'$\\sigma_{\\text{LSR, 1}}$')\n",
    "plt.xlabel('RA')\n",
    "plt.ylabel('DEC')\n",
    "plt.xlim(90,448-90)\n",
    "plt.ylim(90,448-90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## [INPUT] required in this block\n",
    "## Input the directory and name of the output files\n",
    "\n",
    "from astropy.io import fits\n",
    "\n",
    "hdu_output = fits.PrimaryHDU()\n",
    "\n",
    "for i in range(len(list(hdu.header.keys()))):\n",
    "    key = list(hdu.header.keys())[i]\n",
    "    if '3' not in key and '4' not in key and 'COMMENT' not in key and 'HISTORY' not in key and key != '':\n",
    "        # print(key)\n",
    "        hdu_output.header.update({key:hdu.header[key]})\n",
    "hdu_output.header.update({'NAXIS':2})\n",
    "\n",
    "hdu_output.data = mu_ave_peak\n",
    "hdu_output.header['BUNIT'] = 'km/s'\n",
    "hdu_output.writeto(dir_output+'/ALMA_13CO_velocity_intensity_weighted_peak.fits', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## [INPUT] required in this block\n",
    "## Input the directory and name of the output files\n",
    "\n",
    "from astropy.io import fits\n",
    "\n",
    "hdu_output = fits.PrimaryHDU()\n",
    "\n",
    "\n",
    "for i in range(len(list(hdu.header.keys()))):\n",
    "    key = list(hdu.header.keys())[i]\n",
    "    if '3' not in key and '4' not in key and 'COMMENT' not in key and 'HISTORY' not in key and key != '':\n",
    "        # print(key)\n",
    "        hdu_output.header.update({key:hdu.header[key]})\n",
    "hdu_output.header.update({'NAXIS':2})\n",
    "\n",
    "hdu_output.data = A_ave_peak\n",
    "hdu_output.header['BUNIT'] = 'K'\n",
    "hdu_output.writeto(dir_output+'/ALMA_13CO_amplitude_intensity_weighted_peak.fits', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## [INPUT] required in this block\n",
    "## Input the directory and name of the output files\n",
    "\n",
    "from astropy.io import fits\n",
    "\n",
    "hdu_output = fits.PrimaryHDU()\n",
    "hdu_output.data = sigma_ave_peak\n",
    "\n",
    "for i in range(len(list(hdu.header.keys()))):\n",
    "    key = list(hdu.header.keys())[i]\n",
    "    if '3' not in key and '4' not in key and 'COMMENT' not in key and 'HISTORY' not in key and key != '':\n",
    "        # print(key)\n",
    "        hdu_output.header.update({key:hdu.header[key]})\n",
    "\n",
    "hdu_output.header.update({'NAXIS':2})\n",
    "hdu_output.header['BUNIT'] = 'km/s'\n",
    "hdu_output.writeto(dir_output+'/ALMA_13CO_sigma_sum_intensity_weighted_peak.fits', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Velocity gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import leastsq\n",
    "\n",
    "# A plane defined by: z = ax + by +c; output: z\n",
    "def plane(x, y, params):\n",
    "    a, b, c = params\n",
    "    return a * x + b * y + c\n",
    "\n",
    "# Define the difference between a point and the point on the plane with same x and y coordinates\n",
    "def error(params, x, y, z):\n",
    "    return plane(x, y, params) - z\n",
    "\n",
    "# Find the plane that best fits the data\n",
    "def fit_plane(data, x_unit=1, y_unit=1):\n",
    "    # data: a 2d array, data to fit the plane\n",
    "    # x_unit: length in x of every pixel\n",
    "    # y_unit: length in y of every pixel\n",
    "\n",
    "    # Get the shape of the input data\n",
    "    rows, cols = data.shape\n",
    "    \n",
    "    # Generate x, y coordinates\n",
    "    x, y = np.meshgrid(np.arange(cols)*x_unit, np.arange(rows)*y_unit)\n",
    "    \n",
    "    # Flatten the arrays and filter out NaN values\n",
    "    x_flat = x.flatten()\n",
    "    y_flat = y.flatten()\n",
    "    z_flat = data.flatten()\n",
    "    \n",
    "    # Mask for valid (non-NaN) values\n",
    "    mask = ~np.isnan(z_flat)\n",
    "    \n",
    "    x_valid = x_flat[mask]\n",
    "    y_valid = y_flat[mask]\n",
    "    z_valid = z_flat[mask]\n",
    "    \n",
    "    # Initial guess for the parameters a, b, c\n",
    "    initial_guess = [0, 0, np.nanmean(data)]\n",
    "    \n",
    "    # Perform least squares fitting\n",
    "    params, _ = leastsq(error, initial_guess, args=(x_valid, y_valid, z_valid))\n",
    "    \n",
    "    return params, x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the position angle and size of the velocity gradients for plane z = ax + by +c\n",
    "\n",
    "def position_angle(a, b):\n",
    "    # Input: a, b are the parameters of the plane z = ax + by +c\n",
    "    # Output: float, unit in degrees\n",
    "    #   range: -90 to 270, with x-axis being 0 and angle counted in the anticlockwise direction\n",
    "    if -a >= 0:\n",
    "        return 180*np.arctan(b/a)/np.pi\n",
    "    else:\n",
    "        return 180+180*np.arctan(b/a)/np.pi\n",
    "    \n",
    "def slope_plane(a, b):\n",
    "    return np.sqrt(pow(a,2)+pow(b,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get every nth element in a list\n",
    "def get_every_nth(myList, n, default=None):\n",
    "    return np.array([myList[i] for i in range(0, len(myList), n)] or default)\n",
    "\n",
    "# Get every nth element in both x and y directions in a 2d array\n",
    "def get_every_nth_2d(arr, n):\n",
    "    return arr[::n, ::n]\n",
    "\n",
    "# Find the x and y components of an arrow given its amplitude and angle\n",
    "def polar_to_cartesian(amplitude, angle):\n",
    "    # amplitude: amplitude of the arrow\n",
    "    # angle: arrow of the angle from x-axis (in radians)\n",
    "    x = amplitude * np.cos(angle)\n",
    "    y = amplitude * np.sin(angle)\n",
    "    return x, y\n",
    "\n",
    "# Find the amplitude and angle of an arrow given its x and y components\n",
    "def cartesian_to_polar(x, y):\n",
    "    amplitude = np.sqrt(x**2 + y**2)\n",
    "    angle = np.arctan2(y, x)\n",
    "    return amplitude, angle\n",
    "\n",
    "# Averaging the arrows overing a region of m by m pixels\n",
    "def average_vectors(amplitude_array, angle_array, m):\n",
    "    # amplitude_array: 2d n by n square array, each element corresponds to amplitude of the arrow at that pixel\n",
    "    # angle_array: 2d n by n square array, each element corresponds to angle of the arrow at that pixel, in radians\n",
    "    # m: the length of the square regions over which the arrows are averaged\n",
    "    # Note: n needs to be divisible by m\n",
    "\n",
    "    n = amplitude_array.shape[0]\n",
    "    assert n % m == 0, \"n must be divisible by m\"\n",
    "    \n",
    "    # Output size\n",
    "    out_size = n // m\n",
    "    \n",
    "    # Initialize the output arrays\n",
    "    avg_amplitude = np.zeros((out_size, out_size))\n",
    "    avg_angle = np.zeros((out_size, out_size))\n",
    "    \n",
    "    for i in range(out_size):\n",
    "        for j in range(out_size):\n",
    "            # Extract the m x m block\n",
    "            amp_block = amplitude_array[i*m:(i+1)*m, j*m:(j+1)*m]\n",
    "            angle_block = angle_array[i*m:(i+1)*m, j*m:(j+1)*m]\n",
    "            \n",
    "            # Convert to Cartesian coordinates\n",
    "            x_block, y_block = polar_to_cartesian(amp_block, angle_block)\n",
    "            \n",
    "            # Average the Cartesian coordinates\n",
    "            avg_x = np.mean(x_block)\n",
    "            avg_y = np.mean(y_block)\n",
    "            \n",
    "            # Convert back to polar coordinates\n",
    "            avg_amp, avg_ang = cartesian_to_polar(avg_x, avg_y)\n",
    "            \n",
    "            # Store in the output arrays\n",
    "            avg_amplitude[i, j] = avg_amp\n",
    "            avg_angle[i, j] = avg_ang\n",
    "    \n",
    "    return avg_amplitude, avg_angle\n",
    "\n",
    "# Example usage:\n",
    "n = 6\n",
    "m = 2\n",
    "amplitude_array = np.random.rand(n, n)\n",
    "angle_array = np.random.rand(n, n) * 2 * np.pi\n",
    "\n",
    "X, Y = np.meshgrid(np.arange(0,n), np.arange(0,n))\n",
    "plt.quiver(X, Y, np.cos(angle_array)*amplitude_array, np.sin(angle_array)*amplitude_array)\n",
    "\n",
    "X_new, Y_new = np.meshgrid(np.arange(0,n,m), np.arange(0,n,m))\n",
    "avg_amplitude, avg_angle = average_vectors(amplitude_array, angle_array, m)\n",
    "plt.quiver(X_new, Y_new, np.cos(avg_amplitude)*avg_amplitude, np.sin(avg_amplitude)*avg_amplitude, color='blue')\n",
    "\n",
    "print(\"Averaged Amplitude:\\n\", avg_amplitude)\n",
    "print(\"Averaged Angle:\\n\", avg_angle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## [INPUT] required in this block\n",
    "## Input the half length of the square region to fit the plane,\n",
    "##   a plane is fitted to a square with length of 2*halfsize_plane pixels\n",
    "## Note: usually halfsize_plane should be larger than the ratio of the beam major axis and the length of each pixel for results to make sense\n",
    "\n",
    "halfsize_plane = 8\n",
    "\n",
    "slope_ave = np.zeros(shape=(cube_data.shape[1],cube_data.shape[2]),dtype='float')\n",
    "position_angle_ave = np.zeros(shape=(cube_data.shape[1],cube_data.shape[2]),dtype='float')\n",
    "v_plane_ave = np.zeros(shape=(cube_data.shape[1],cube_data.shape[2]),dtype='float')\n",
    "\n",
    "\n",
    "for xchan in range(0+halfsize_plane, 448-halfsize_plane):\n",
    "    for ychan in range(0+halfsize_plane, 448-halfsize_plane):\n",
    "        data = mu_ave_peak[ychan-halfsize_plane:ychan+halfsize_plane,xchan-halfsize_plane:xchan+halfsize_plane]\n",
    "        if len([1 for x in data.reshape(len(data)*len(data[0])) if math.isnan(x) == False]) > pow(halfsize_plane, 2)/2:\n",
    "            params, x, y = fit_plane(data, x_unit=abs(dx)*deg_to_pc, y_unit=abs(dy)*deg_to_pc)\n",
    "            a, b, c = params\n",
    "\n",
    "            # Calculate the overall slope (magnitude of the gradient)\n",
    "            slope_ave[ychan, xchan] = np.sqrt(a**2 + b**2)\n",
    "            position_angle_ave[ychan, xchan] = position_angle(a, b)\n",
    "            v_plane_ave[ychan, xchan] = c\n",
    "        else:\n",
    "            slope_ave[ychan, xchan] = np.nan\n",
    "            position_angle_ave[ychan, xchan] = np.nan   \n",
    "            v_plane_ave[ychan, xchan] = np.nan           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## [INPUT] required in this block\n",
    "## Input the half length of region plotted: a square with length of 2*halfsize_plot is plotted\n",
    "## Input the half length of region averaged: the arrows are averaged for a square region with length sep_pixel\n",
    "## Note: 2*halfsize_plot needs to be divisible by sep_pixel\n",
    "\n",
    "halfsize_plot = 100\n",
    "sep_pixel = 8\n",
    "\n",
    "# Create a grid of data\n",
    "X, Y = np.meshgrid(np.arange(xcenter-halfsize_plot+round(sep_pixel/2), xcenter+halfsize_plot+round(sep_pixel/2), sep_pixel), np.arange(ycenter-halfsize_plot+round(sep_pixel/2), ycenter+halfsize_plot+round(sep_pixel/2), sep_pixel))\n",
    "\n",
    "pa_init = (position_angle_ave[ycenter-halfsize_plot:ycenter+halfsize_plot, xcenter-halfsize_plot:xcenter+halfsize_plot])/180*np.pi\n",
    "c_init = slope_ave[ycenter-halfsize_plot:ycenter+halfsize_plot, xcenter-halfsize_plot:xcenter+halfsize_plot]\n",
    "\n",
    "avg_amplitude, avg_angle = average_vectors(c_init, pa_init, sep_pixel)\n",
    "v_plot = get_every_nth_2d(mu_ave_peak[ycenter-halfsize_plot+round(sep_pixel/2):ycenter+halfsize_plot+round(sep_pixel/2), xcenter-halfsize_plot+round(sep_pixel/2):xcenter+halfsize_plot+round(sep_pixel/2)], sep_pixel)\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the magnitude and direction of the arrows\n",
    "# Positive: pointing from higher velocity to lower velocity\n",
    "print('shape of objects (have to be equal):', X.shape, Y.shape, avg_amplitude.shape, avg_angle.shape, v_plot.shape)\n",
    "\n",
    "# Plot the arrows\n",
    "fig, ax = plt.subplots(figsize=(10,10), subplot_kw={'projection':wcs2d})\n",
    "im = ax.quiver(X, Y, np.cos(avg_angle)*np.sqrt(avg_amplitude), np.sin(avg_angle)*np.sqrt(avg_amplitude), v_plot, norm=colors.Normalize(vmin=mu_low,vmax=mu_high),cmap='coolwarm', headaxislength=2, headlength=2)\n",
    "ax.set_aspect('equal')\n",
    "cbar = fig.colorbar(im, fraction=0.03)\n",
    "cbar.set_label(label=r'$v_{\\text{LSR}}$ [km s$^{-1}$]',size=10)\n",
    "\n",
    "# Set the axes limits\n",
    "plt.xlim(xcenter - halfsize_plot,xcenter + halfsize_plot)\n",
    "plt.ylim(ycenter - halfsize_plot,ycenter + halfsize_plot)\n",
    "\n",
    "plt.xlabel('RA', size=10)\n",
    "plt.ylabel('DEC', size=10)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VSCode",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
